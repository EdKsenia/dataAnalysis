Читаем данные.
1) Подключаем необходимые библиотеки.
2) Загружаем необходимые для анализа данные (ORGANICS)
3) Прсмотрим по каким параметрам оцениваются данные. Кол-во изучаемыз объектов = 22223 и кол-во признаков = 13.
При этом узнаем что среди значений признаков встречаюся пустые значения. 
Соответственно нам необходимо избиваться от таких значений.
4)Первый способ это удаление пустых значений dropna()). При этом кол-во объектов уменьшилось до 16408, что 
состоявляет примерно 70% от изначального датасета.
Так же мы можем заменить наши пропуски , без потери данных. Два способа: заменить на среднее значение
(fillna(df.mean())) или же на наиболее часто встречающиеся значения (fillna(df.mod)). Заменются
только числовые значения, т.е останутся пустые значения где формат object.

Начинаем обучение модели.
1)Делим на обучающую(train) и тестовыую(test) выборку при помощи train_test_split.
2) Создаем дерево и обучаем его.
3) Для того чтобы нам посмотреть результаты, визуализируем модели, используя export_graphviz.
4) Так же необходимо определить насколько точна модель. Результаты точности на обучающей выборке = 0,80,
а на тестовой = 0,79
5)Посмотри результаты оценки модели в зависимости от глубины дерева, выведем их в таблицу. При увеличени глубины, 
увеличивалась и вероятность, но на глубине 7 число правильных результатов на тестовой выборке уменьшилось. 
Чтобы это посомтреть наглядно постоим график, на котором виду что результаты обучающей выборки выше и на изучаемой 
глубине увеличиваются, тогда как на тестовой они ниже, и при этом в какой-то момент начинают ухудшаться.
